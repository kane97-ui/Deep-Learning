**MSE VS CE**: 对于分类任务来说，选择cross entropy
**原因**：

* MSE 在分类任务中的表现没有CE准确
* MSE 在神经网络模型中的目标函数是非凸函数，而cross entropy是凸函数。所以在优化难度上，CE更小。而MSE很容易陷入局部最优。

**generalization：**
$Er_{out}\leq Er_{in}+O({d \over \sqrt{n}}) $
**underfitting： $Er_{in}$过大**

* 模型太过简单
* 在优化过程中，陷入了局部最优或者很难找到全局最优
* 计算资源的限制（没有足够的迭代次数）

**Overfitting ：$O({d \over \sqrt{n}})$ 过大**

* vc demension 太大或者说H太大
* 训练数据量太小
* 数据很容易受到噪声的影响，所以我们学到的模型很可能去描述噪声带来的特性，而不是真是数据本身的特性。

**注意：基于机器学习理论，$Er_{in}$ 本身是不会造成overfitting的，也就是说，在固定d和N的情况下，无限优化$Er_{in}$，让它减小，$Er_{out}$也是会减小的。但在深度学习模型训练中，随着迭代次数的增加或者epoch的增加，是有可能造成overfitting的原因在于：神经网络可能由于dropout方法，造成的d的改变，或者其他原因。**

**减小模型能力（capacity）或者说克服overfitting的方法：**
* 减小数据的特征
* 减小相互独立的参数的数量
* 减小模型的尺寸
* 减小训练迭代次数（在机器学习理论下是错误的）
* 添加正则化
  

  


