### 多层感知机
* 线性可分模型：两层神经网络是可以解决的（输入和输出）
* 线性不可分模型：需要增加一层或多层隐藏层，再加上激活函数

**激活函数的目的：**
* 线性回归的组合也是线性回归，是不可以用作非线性问题的，神经网络的激活函数可以让模型非线性化
* 激活函数在输出层可以起到映射输出值的作用，如：sigmoid函数可以让输出值映射到0-1
  
**三层神经网络的作用：**
* 它可以代表任何目标函数
* 如果想让他代表任何函数，隐藏层的神经元必须非常大
* 其实SVM，KNN，Boosting可以看成拥有1层或者2层隐藏层的神经网络
* 隐藏层的神经元个数是可以通过增加网络深度而指数减小：所以在构造神经网络时，更愿意深度上的增加，而不是宽度上的扩张。因为这个可以减少神经元的个数，从而减少参数的个数，从而减少计算量。

**Mini-batch based SGD:**
* 将训练数据分成很多小的batches
* 在每次epoch，会随机分成很多batches，每个epoch就等于是对所有数据集单独的解释
* 开始会设很小的batch size，然后增加size随着训练的进行：防止出现局部最优，因为每次mini batch不一样，方向梯度不一样。开始size设的很小，是为了更大的可能去探索不同的方向。而训练到后面让数据集变大是为了精确定位最终的优化方向。
  
**随机梯度下降优点和缺点：**
* 梯度的估计是有噪声的，梯度可能不是朝着正确的方向进行的
* 但比batch-learnig更快
* 梯度估计的噪声也有可能会带来好的结果：防止局部最优
* 因为嘛每次训练一个mini-batch或者一个数据，所以每次的梯度是在变化的，所以权重也是在浮动的，所以可能很难收敛

**Batch learning优点和缺点：**
* 收敛条件非常容易理解
* 一些加速技巧是在batch learning上操作的
* 权重变化和收敛速度的理论分析更简单
  
**激活函数必须具有的性质：**
* 必须是非线性的
* 输出必须是有界的：保证权重和激活值是有界的，限制了训练时间
* 必须是连续可微的
* 在某一部分是线性的（sigmoid的在[-2,2]是接近线性的）保证了网络可以实现线性模型。

**全局表征和局部表征：**
* Tanh 激活函数的特征
  * 相对sigmoid函数在更大区域上有响应（任何输入在Tanh函数上都可能被激活）这也叫distributed或者global representation
  * 如果输入只有很小的范围在激活函数上被激活-local representation
  * Distributed representation 更好，因为更多的数据会对后面的网络产生影响。
  * 如果网络的layer很多，那么global representation是更好的选择
* 
